<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ashan Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://ashan2012.github.info/"/>
  <updated>2017-12-16T12:40:17.000Z</updated>
  <id>http://ashan2012.github.info/</id>
  
  <author>
    <name>SuperAshan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>源码阅读系列之textrank</title>
    <link href="http://ashan2012.github.info/2017/02/21//blog/2017/02/21/textrank%E8%A7%A3%E8%AF%BB.html/"/>
    <id>http://ashan2012.github.info/2017/02/21//blog/2017/02/21/textrank解读.html/</id>
    <published>2017-02-20T16:00:00.000Z</published>
    <updated>2017-12-16T12:40:17.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。</p></blockquote><p>网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。<br>Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextRAnk4Keyword.py主要用于提取关键词，TextRank4Sentence.py主要抽取关键的句子。util.py主要是关键词和句子排序。</p><p>先看提取关键词:<br>TextRank4Keyword<br><strong>init</strong>.初始化.可以在stopword.txt中添加停止词<br>进入主函数analyze中，传入参数包括要进行提取的原文，关键词边的窗口大小，是够都变为小写,节点的类型选项（影响分词结果),边的类型，pagerank的参数。</p><p>首先分词result = self.seg.segment(text=text, lower=lower).分词函数在Segmentation中.Segmentation初始化两个级别的分词项,WordSegmentation,SentenceSegmentation,词级别和句子级别。句子通过util.delimiters直接切割，然后词语切割直接调用结巴分词，具体结巴分词介绍可以参照:<a href="https://ashan2012.github.io/blog/2016/06/15/jieba%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB.html" title="源码阅读之jieba分词" target="_blank" rel="noopener">源码阅读之结巴分词</a>。<br>分词的时候通过allow_speech_tags控制生成词语的词性，返回的结果为dict，嵌套三层结构。第一层为选项，主要是是否使用停用词，和是否使用词性过滤两种，第二层对应句子，每个句子为word_list.</p><p>分词之后就进行图的构造，图构造然后排序函数为:util.sort_words.进入util.py文件中，sort_words函数中，首先是构造图:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word_list <span class="keyword">in</span> _vertex_source:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> word_list:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> word <span class="keyword">in</span> word_index:</span><br><span class="line">            word_index[word] = words_number</span><br><span class="line">            index_word[words_number] = word</span><br><span class="line">            words_number += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">graph = np.zeros((words_number, words_number))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word_list <span class="keyword">in</span> _edge_source:</span><br><span class="line">    <span class="keyword">for</span> w1, w2 <span class="keyword">in</span> combine(word_list, window):</span><br><span class="line">        <span class="keyword">if</span> w1 <span class="keyword">in</span> word_index <span class="keyword">and</span> w2 <span class="keyword">in</span> word_index:</span><br><span class="line">            index1 = word_index[w1]</span><br><span class="line">            index2 = word_index[w2]</span><br><span class="line">            graph[index1][index2] = <span class="number">1.0</span> </span><br><span class="line">            graph[index2][index1] = <span class="number">1.0</span></span><br></pre></td></tr></table></figure><p>其中combine是构造边的函数，combine函数:<br>combine函数为word_list,中窗口window中的word之间有边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combine</span><span class="params">(word_list, window = <span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""构造在window下的单词组合，用来构造单词之间的边。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Keyword arguments:</span></span><br><span class="line"><span class="string">    word_list  --  list of str, 由单词组成的列表。</span></span><br><span class="line"><span class="string">    windows    --  int, 窗口大小。</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> window &lt; <span class="number">2</span>: window = <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">1</span>, window):</span><br><span class="line">        <span class="keyword">if</span> x &gt;= len(word_list):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        word_list2 = word_list[x:]</span><br><span class="line">        res = zip(word_list, word_list2)</span><br><span class="line">        <span class="keyword">for</span> r <span class="keyword">in</span> res:</span><br><span class="line">            <span class="keyword">yield</span> r</span><br></pre></td></tr></table></figure><p>构造好图矩阵之后，调用pagerank算法，使用Python中的模块networkx：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nx_graph = nx.from_numpy_matrix(graph)</span><br><span class="line">scores = nx.pagerank(nx_graph, **pagerank_config)</span><br></pre></td></tr></table></figure><p>结果为排好序的节点（即词语)</p><p>提取关键句子，（自动摘由抽取式的)</p><p>流程和提取关键词的流程完全一致,只不过构造图的时候边的权重为两个句子的相似度，两个句子相似度函数为getsimmi</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_similarity</span><span class="params">(word_list1, word_list2)</span>:</span></span><br><span class="line">    <span class="string">"""默认的用于计算两个句子相似度的函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Keyword arguments:</span></span><br><span class="line"><span class="string">    word_list1, word_list2  --  分别代表两个句子，都是由单词组成的列表</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    words   = list(set(word_list1 + word_list2))</span><br><span class="line">    vector1 = [float(word_list1.count(word)) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line">    vector2 = [float(word_list2.count(word)) <span class="keyword">for</span> word <span class="keyword">in</span> words]</span><br><span class="line"></span><br><span class="line">    vector3 = [vector1[x]*vector2[x]  <span class="keyword">for</span> x <span class="keyword">in</span> xrange(len(vector1))]</span><br><span class="line">    vector4 = [<span class="number">1</span> <span class="keyword">for</span> num <span class="keyword">in</span> vector3 <span class="keyword">if</span> num &gt; <span class="number">0.</span>]</span><br><span class="line">    co_occur_num = sum(vector4)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> abs(co_occur_num) &lt;= <span class="number">1e-12</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    denominator = math.log(float(len(word_list1))) + math.log(float(len(word_list2))) <span class="comment"># 分母</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> abs(denominator) &lt; <span class="number">1e-12</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> co_occur_num / denominator</span><br></pre></td></tr></table></figure><p>公式为: [co_word_num/(log(list1num)+log(list2num))]</p><p>改进的方法：<br>根据pagerank的算法，可以改进的地方就是图的边，词语之间，句子之间的权重，可以使用word2vector训练得到词向量，结合位置关系计算他们之间的关系。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。&lt;/p&gt;
&lt;/blockqu
      
    
    </summary>
    
      <category term="源码阅读" scheme="http://ashan2012.github.info/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="textrank" scheme="http://ashan2012.github.info/tags/textrank/"/>
    
      <category term="关键词" scheme="http://ashan2012.github.info/tags/%E5%85%B3%E9%94%AE%E8%AF%8D/"/>
    
      <category term="关键词提取" scheme="http://ashan2012.github.info/tags/%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8F%90%E5%8F%96/"/>
    
      <category term="关键句子提取" scheme="http://ashan2012.github.info/tags/%E5%85%B3%E9%94%AE%E5%8F%A5%E5%AD%90%E6%8F%90%E5%8F%96/"/>
    
      <category term="自动摘要" scheme="http://ashan2012.github.info/tags/%E8%87%AA%E5%8A%A8%E6%91%98%E8%A6%81/"/>
    
      <category term="摘要生成" scheme="http://ashan2012.github.info/tags/%E6%91%98%E8%A6%81%E7%94%9F%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>工作笔记之一：猜你喜欢模块</title>
    <link href="http://ashan2012.github.info/2017/01/13//blog/2017/01/13/guesswantquery.html/"/>
    <id>http://ashan2012.github.info/2017/01/13//blog/2017/01/13/guesswantquery.html/</id>
    <published>2017-01-12T16:00:00.000Z</published>
    <updated>2017-12-16T12:44:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="猜你喜欢"><a href="#猜你喜欢" class="headerlink" title="猜你喜欢"></a>猜你喜欢</h2><p>推荐语料为搜索query，目标为提升搜索量，提升用户搜索体验，分解一下为推出的query为相关和惊喜度，+热门<br>整体方案如下：<br><img src="http://ashan2012.github.io/images/guesswantquery/structure.png" alt="猜你喜欢数据流程图"></p><h3 id="召回阶段"><a href="#召回阶段" class="headerlink" title="召回阶段"></a>召回阶段</h3><hr><p><strong>1. 种子集合</strong></p><ul><li>搜索的query，按照搜索的频度可以分为经常搜索的和最近搜索的query</li><li>用户的画像(可以根据用户的其他行为),最好对于用户总结关键词，tag，类别等分层次的行为</li></ul><p><strong>2. 扩展候选集合</strong></p><ul><li>扩展可以根据item-base,query之间的关系扩展</li><li>扩展可以根据用户的关键词图谱召回相关query</li><li>扩展可以通过tag和类别召回惊喜度较高的query</li><li>有些词语不进行扩展，专门的词库:天气，彩票，双色球等等</li></ul><p><strong>3.低质量过滤</strong></p><ul><li>第一层为query的UV过滤<ul><li>uv过滤根据query的搜索UV，删掉搜索人数比较少的query，砍掉80%的query</li></ul></li><li>第二层基于词表的过滤   <ul><li><strong>全量匹配</strong>：全量是硬匹配，一些事件，人物，垃圾词等等。“蛤蛤怒斥香港女记者”，“身度完整版”，“6080神马影院”，“东条希本子”</li><li><strong>部分匹配</strong>:一些垃圾词，色情，隐私等</li><li><strong>乱码</strong>：可能是韩文，日文，搜索日志中的乱码。</li><li><strong>政治敏感词</strong>:政治敏感词库不能出</li><li><strong>竞品词</strong>：不让出的</li></ul></li><li>第二层为基于libshorttxt的二分类模型<ul><li>以字为特征，构造正例和负例。其中正例为热门中的正常query。负例为匹配出来的query，两边都是1万</li></ul></li></ul><p><strong>4.去重</strong></p><p>  由于用户本身搜索的query表明的意思比较集中，则通过扩展构造的query集合大量重复的query，这种大量重复的query影响了用户体验。所以需要去重逻辑，保证推出的query的多样性</p><ul><li><p>第一层去重逻辑为长词的归一化</p><p>归一化的目的是合并一些表明一个意思的词语到一个意思，类似于query聚类，然后去类的中心点。聚类通过query的点击url的关系来实现，实现方法是将点击一个url（这个url必须是落地页面）的query中找到一个keyquery，keyquery的计算方法是用query的点击次数/query的长度，然后将二者的关系作为key,value,相同的key选value最大的为keyquery</p><ul><li>归一化比较规范的:<ul><li>00后时髦小女生图片  REPLACE 00后小美女/2.4</li><li>贷视频流出 REPLACE 裸贷肉偿视频遭疯传/8.5 不同地方热点事件归一化到最规范的说法</li><li>12360下载官方下载手机版 REPLACE 12360下载/175.111111111</li><li>qq飞车星光秀场怎么进去  REPLACE 星光秀场/114.75</li><li>奥运会项目有哪些    REPLACE 奥运会/55.6</li><li>芭比公主之美人鱼的故事  REPLACE 美人鱼故事/4.6</li><li>八零后副厅  REPLACE 王玺玮的父亲/4.0</li><li>保卫萝卜2攻略108攻略水晶萝卜头  REPLACE 保卫萝卜108/3.63636363636</li><li>保卫部落3d策略塔防  REPLACE 保卫部落/4.0</li></ul></li><li>也有代表并不正确：<ul><li>07excel箭头在哪 REPLACE excel绘图工具在哪/3.64705882353</li><li>09年君威    REPLACE 09年君威二手车价格/478.75</li><li>0p手机剩余电量怎么设置  REPLACE 小米手机电量百分比/7.88888888889</li></ul></li></ul></li><li>第二层为一些case的规则:<ul><li>比如说对小说多个章节，电影多个系列的合并，通过正则表达式[\d]+?字，部，章等<ul><li>三妹53集        NEW:三妹</li><li>小雄的故事1864章        NEW:小雄的故事</li></ul></li><li>根据一些词语切割，取前面的意思：<ul><li>OLD:0852蟹总完整版     NEW:0852蟹总</li><li>OLD:2345影视大全       NEW:2345影视</li><li>gta5秘籍大全       NEW:gta5秘籍</li><li>傲世掌控txt下载    NEW:傲世掌控</li><li>爱莎美瞳官网       NEW:爱莎美瞳</li><li>霸皇下载   NEW:霸皇</li></ul></li><li>针对小说漫画，针对用户构成，对小说，漫画单独处理，根据小说的名字，漫画的名字，每个中最多出两个:<ul><li>择天记电视剧-择天记</li><li>偷香高手||5||0.13032||712   偷香高手类似的</li><li>绝世兵王五十二策  绝世兵王</li></ul></li><li>包含关系<ul><li>包含关系留下UV大的</li></ul></li></ul></li><li><p>第三层是根据一些字面的意思：但是query1+query2的长度必须小于9，不做处理，错误率比较高</p><ul><li><p>字集合的jaccad系数和提取关键词的集合jaccad系数(关键词定义为<strong><em>名词，动词，成语，关键词</em></strong>集合中的jaccard系数)</p><ul><li><p><strong>正确的</strong></p><ul><li>6s运行内存多大    苹果运行内存多大        0.750   0.750</li><li>dmi指标详解       dmi指标详解视频 0.778   0.778</li><li>阿笠博士的幕后故事        阿笠博士幕后故事17</li><li>穿越火线乔杉电影  穿越火线电影    0.750   0.750</li><li>初中生综合素质评价        初中生综合素质评语 </li><li>荣耀magic视频     荣耀magic       0.778   0.778</li></ul></li><li><p><strong>错误的</strong>      </p><ul><li>超级农场  超级农场主      0.800   0.800</li><li>大明星的贴身护卫  大小姐的贴身护卫        0.750   0.750</li><li>都市仙君  都市大仙君      0.800   0.800</li></ul></li></ul></li><li><p>语义层面的去重，通过wordtovector训练term之间的关系，主要解决的问题是短文本，query1+query2的长度&lt;9的词语，一些明显关联度比较高的</p><ul><li>羽衣传说||5||0.20555||1373—羽衣传说目录||5||0.20319||1715      0.816</li><li>于成龙||5||0.10566||62266—于成龙电视剧||2||2||111943   0.780</li><li>爱恨何欢||8||0.17406||44—爱恨何欢欣欣向荣||3||10||45   0.882</li><li>冷酷总裁的一夜情人||5||0.10671||1920—总裁的囚宠情人||2||2||9933    0.832</li><li>天才魔妃太难追||2||3||2706—天才魔妃我要了||5||0.10264||6309    0.799</li><li>红色鞋子||8||0.08283||52—鞋子||3||3||223   0.779</li><li>夜太黑||8||0.09022||182—夜太黑林忆莲||3||10||394   0.850</li><li>冰火魔厨||3||3||8336—冰火魔厨漫画||8||0.21497||21585   0.866</li><li>洪荒之证道不朽||8||0.09329||244—洪荒不朽||8||0.10211||3093 0.880</li><li>星际全能女王||8||0.08964||14—星际女神攻略||3||3||1335  0.792</li><li>洛天依||3||5||11415—洛天依吸毒||8||0.22113||45463  0.725</li><li>捕鱼大师||5||0.23225||758—捕鱼大师破解版||2||2||854 </li><li>蛇蛇大作战无敌版||8||0.10439||2642—贪吃蛇大作战||8||0.10919||35958 0.816</li></ul></li></ul></li></ul><h3 id="排序阶段"><a href="#排序阶段" class="headerlink" title="排序阶段"></a>排序阶段</h3><hr><p>排序阶段一般采用learning to rank,或者ctr预估的模型，这个后续再写，第一版的版本就是按照规则排序的。聚类之后打散的过程，按照常搜在前面的内容       </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;猜你喜欢&quot;&gt;&lt;a href=&quot;#猜你喜欢&quot; class=&quot;headerlink&quot; title=&quot;猜你喜欢&quot;&gt;&lt;/a&gt;猜你喜欢&lt;/h2&gt;&lt;p&gt;推荐语料为搜索query，目标为提升搜索量，提升用户搜索体验，分解一下为推出的query为相关和惊喜度，+热门&lt;br&gt;整体方
      
    
    </summary>
    
      <category term="工作笔记" scheme="http://ashan2012.github.info/categories/%E5%B7%A5%E4%BD%9C%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="推荐" scheme="http://ashan2012.github.info/tags/%E6%8E%A8%E8%8D%90/"/>
    
      <category term="短文本" scheme="http://ashan2012.github.info/tags/%E7%9F%AD%E6%96%87%E6%9C%AC/"/>
    
      <category term="去重" scheme="http://ashan2012.github.info/tags/%E5%8E%BB%E9%87%8D/"/>
    
      <category term="反垃圾" scheme="http://ashan2012.github.info/tags/%E5%8F%8D%E5%9E%83%E5%9C%BE/"/>
    
  </entry>
  
  <entry>
    <title>源码阅读系列之word2vector</title>
    <link href="http://ashan2012.github.info/2016/12/14//blog/2016/12/14/word2vector%E9%98%85%E8%AF%BB.html/"/>
    <id>http://ashan2012.github.info/2016/12/14//blog/2016/12/14/word2vector阅读.html/</id>
    <published>2016-12-13T16:00:00.000Z</published>
    <updated>2017-12-16T13:19:39.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本的参数介绍"><a href="#基本的参数介绍" class="headerlink" title="基本的参数介绍"></a>基本的参数介绍</h2><ul><li><strong>size</strong> 输出的vector的维度</li><li><strong>train</strong> 训练的文本，已经分好词，按照所有的行分割，一行最多10000个词</li><li><strong>save-vocab</strong> 将文本中读取的词出现次数保存</li><li><strong>read-vacab</strong> 不从训练的文本读取词次数，直接从文件中读取词的次数</li><li><strong>debug</strong> 输出更多信息</li><li><strong>binary</strong> 以二进制的方式保存模型，其实就是每个词语对应的vector</li><li><strong>cbow</strong>是否使用cbow模型</li><li><strong>alpha</strong> 梯度下降的初始学习率，默认的skip-gram 0.025,cbow 0.05,实际是每处理10000个词，   alpha慢慢减小</li><li><strong>window</strong> 前后词的窗口最大长度(context(w))的长度，实际会有一个0-window的随机</li><li><strong>sample</strong> 采样的时候高频词会有一定的丢弃概率</li><li><strong>hs</strong> 是否使用Hierarchical Softmax，默认等于0，不使用</li><li><strong>negtive</strong> 负采样词语个数</li><li><strong>threads</strong> 线程个数，默认12</li><li><strong>iter</strong> 迭代次数，默认是5</li><li><strong>min-count</strong> 低于min-count的词语直接丢弃</li><li><strong>classes</strong> 输出的是word的类别，不是vector</li></ul><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p><strong>./word2vec -train data.txt -output vec.txt -size 200 -window 5 -sample 1e-4 -negative 5 -hs 0 -binary 0 -cbow 1 -iter 3</strong></p><ol><li>tricks 提前算好sigmod函数:</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">expTable = (real *)<span class="built_in">malloc</span>((EXP_TABLE_SIZE + <span class="number">1</span>) * <span class="keyword">sizeof</span>(real));</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; EXP_TABLE_SIZE; i++) &#123;</span><br><span class="line">    expTable[i] = <span class="built_in">exp</span>((i / (real)EXP_TABLE_SIZE * <span class="number">2</span> - <span class="number">1</span>) * MAX_EXP);</span><br><span class="line">    expTable[i] = expTable[i] / (expTable[i] + <span class="number">1</span>); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="数据结构介绍"><a href="#数据结构介绍" class="headerlink" title="数据结构介绍"></a>数据结构介绍</h2><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">//softmax存储词语的huffuman编码信息</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">vocab_word</span> </span></span><br><span class="line"><span class="class">&#123;</span>  </span><br><span class="line"> <span class="keyword">long</span> <span class="keyword">long</span> cn; <span class="comment">//词频  </span></span><br><span class="line"> <span class="keyword">int</span> *point; <span class="comment">//huffman编码对应内节点的路径  </span></span><br><span class="line"> <span class="keyword">char</span> *word, *code, codelen;<span class="comment">//huffman编码  </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>数组vocab存取词语信息，每次1000的量增加</p><h2 id="函数介绍"><a href="#函数介绍" class="headerlink" title="函数介绍"></a>函数介绍</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TrainModel()</span><br></pre></td></tr></table></figure><p>读取每个词语，存入vacab，和vacab_hash中.</p><p>从词表中读入信息:<br>读入代码:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">//我觉得此处有问题，为什么没有读入词频</span></span><br><span class="line">vocab[vocab_size].word = (<span class="keyword">char</span> *)<span class="built_in">calloc</span>(length, <span class="keyword">sizeof</span>(<span class="keyword">char</span>))</span><br><span class="line"><span class="built_in">strcpy</span>(vocab[vocab_size].word, word); </span><br><span class="line">vocab[vocab_size].cn = <span class="number">0</span>;</span><br><span class="line">vocab_size++;</span><br></pre></td></tr></table></figure><p>hash采用线性冲突法。顺序往下找:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">hash = GetWordHash(word); </span><br><span class="line"><span class="keyword">while</span> (vocab_hash[hash] != <span class="number">-1</span>) hash = (hash + <span class="number">1</span>) % vocab_hash_size;</span><br><span class="line">vocab_hash[hash] = vocab_size - <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>其中hash值的获得很巧妙:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">GetWordHash</span><span class="params">(<span class="keyword">char</span> *word)</span> </span>&#123;  </span><br><span class="line"><span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> a, hash = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; <span class="built_in">strlen</span>(word); a++) hash = hash * <span class="number">257</span> + word[a];</span><br><span class="line">hash = hash % vocab_hash_size;</span><br><span class="line"><span class="keyword">return</span> hash;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>读取词语结束后，<code>SortVocab</code>所有的词语排序，然后去掉出现次数小于min_count的词语，然后重新调整vacab_hash,同时开辟存储二叉树的节点，路径的排序</p><p>从训练级中读取数据:<br>相同的思路但是需要在词语数目大的时候，删减词语集合</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">if</span> (vocab_size &gt; vocab_hash_size * <span class="number">0.7</span>) ReduceVocab();</span><br><span class="line">   <span class="comment">//默认min_reduce=1</span></span><br><span class="line">   <span class="function"><span class="keyword">void</span> <span class="title">ReduceVocab</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> <span class="keyword">int</span> a, b = <span class="number">0</span>;</span><br><span class="line"> <span class="keyword">unsigned</span> <span class="keyword">int</span> hash;</span><br><span class="line"> <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) <span class="keyword">if</span> (vocab[a].cn &gt; min_reduce) &#123;</span><br><span class="line">   vocab[b].cn = vocab[a].cn;</span><br><span class="line">   vocab[b].word = vocab[a].word;</span><br><span class="line">   b++;</span><br><span class="line"> &#125; <span class="keyword">else</span> <span class="built_in">free</span>(vocab[a].word);</span><br><span class="line"> vocab_size = b;</span><br><span class="line"><span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_hash_size; a++) vocab_hash[a] = <span class="number">-1</span>; </span><br><span class="line"> <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) &#123;</span><br><span class="line">   <span class="comment">// Hash will be re-computed, as it is not actual</span></span><br><span class="line">  hash = GetWordHash(vocab[a].word);</span><br><span class="line">   <span class="keyword">while</span> (vocab_hash[hash] != <span class="number">-1</span>) hash = (hash + <span class="number">1</span>) % vocab_hash_size;</span><br><span class="line">    vocab_hash[hash] = a;</span><br><span class="line"> &#125;</span><br><span class="line"> fflush(<span class="built_in">stdout</span>);</span><br><span class="line"> min_reduce++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>huffuman树的建树很有意思:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">   <span class="comment">//实际写一个6,5,4,3,2,1的过程就明白了</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">CreateBinaryTree</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       <span class="keyword">long</span> <span class="keyword">long</span> a, b, i, min1i, min2i, pos1, pos2, point[MAX_CODE_LENGTH];</span><br><span class="line">       <span class="keyword">char</span> code[MAX_CODE_LENGTH];</span><br><span class="line">       <span class="keyword">long</span> <span class="keyword">long</span> *count = (<span class="keyword">long</span> <span class="keyword">long</span> *)<span class="built_in">calloc</span>(vocab_size * <span class="number">2</span> + <span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">long</span> <span class="keyword">long</span>));</span><br><span class="line">       <span class="keyword">long</span> <span class="keyword">long</span> *binary = (<span class="keyword">long</span> <span class="keyword">long</span> *)<span class="built_in">calloc</span>(vocab_size * <span class="number">2</span> + <span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">long</span> <span class="keyword">long</span>));</span><br><span class="line">       <span class="keyword">long</span> <span class="keyword">long</span> *parent_node = (<span class="keyword">long</span> <span class="keyword">long</span> *)<span class="built_in">calloc</span>(vocab_size * <span class="number">2</span> + <span class="number">1</span>, <span class="keyword">sizeof</span>(<span class="keyword">long</span> <span class="keyword">long</span>));</span><br><span class="line">       <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) count[a] = vocab[a].cn;</span><br><span class="line">       <span class="keyword">for</span> (a = vocab_size; a &lt; vocab_size * <span class="number">2</span>; a++) count[a] = <span class="number">1e15</span>;</span><br><span class="line">       pos1 = vocab_size - <span class="number">1</span>;</span><br><span class="line">       pos2 = vocab_size;</span><br><span class="line">       <span class="comment">// Following algorithm constructs the Huffman tree by adding one node at a time</span></span><br><span class="line">       <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size - <span class="number">1</span>; a++) &#123;</span><br><span class="line">           <span class="comment">// First, find two smallest nodes 'min1, min2'</span></span><br><span class="line">           <span class="keyword">if</span> (pos1 &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">               <span class="keyword">if</span> (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">                   min1i = pos1;</span><br><span class="line">                   pos1--;</span><br><span class="line">               &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                   min1i = pos2;</span><br><span class="line">                   pos2++;</span><br><span class="line">               &#125;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              min1i = pos2;</span><br><span class="line">              pos2++;</span><br><span class="line">          &#125;   </span><br><span class="line">          <span class="keyword">if</span> (pos1 &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">              <span class="keyword">if</span> (count[pos1] &lt; count[pos2]) &#123;</span><br><span class="line">                  min2i = pos1;</span><br><span class="line">                  pos1--;</span><br><span class="line">             &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                 min2i = pos2;</span><br><span class="line">                 pos2++;</span><br><span class="line">             &#125;</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              min2i = pos2;</span><br><span class="line">              pos2++;</span><br><span class="line">          &#125;      </span><br><span class="line">         count[vocab_size + a] = count[min1i] + count[min2i]; <span class="comment">//min1,min2为每次组建树        </span></span><br><span class="line">                                                              <span class="comment">//的叶子节点，其中min2i    </span></span><br><span class="line">                                                              <span class="comment">//为较大的那一个叶子节点</span></span><br><span class="line">         parent_node[min1i] = vocab_size + a;  <span class="comment">//存储父子关系</span></span><br><span class="line">         parent_node[min2i] = vocab_size + a;</span><br><span class="line">         binary[min2i] = <span class="number">1</span>;   <span class="comment">//存储0，1作为路径</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// Now assign binary code to each vocabulary word</span></span><br><span class="line">      <span class="comment">//point太牛逼,point第一位一直为vocab_size-2,为树的根节点，在code中不存储根节点</span></span><br><span class="line">      </span><br><span class="line">      <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) &#123;</span><br><span class="line">        b = a;</span><br><span class="line">        i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">           code[i] = binary[b];</span><br><span class="line">           point[i] = b;</span><br><span class="line">           i++;</span><br><span class="line">           b = parent_node[b];</span><br><span class="line">           <span class="keyword">if</span> (b == vocab_size * <span class="number">2</span> - <span class="number">2</span>) <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        vocab[a].codelen = i;</span><br><span class="line">        vocab[a].point[<span class="number">0</span>] = vocab_size - <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; i; b++) &#123;</span><br><span class="line">            vocab[a].code[i - b - <span class="number">1</span>] = code[b];   <span class="comment">//顺序倒过来，从根节点下一层往下到叶子      </span></span><br><span class="line">                                                  <span class="comment">//节点</span></span><br><span class="line">            vocab[a].point[i - b] = point[b] - vocab_size; <span class="comment">//meikandong</span></span><br><span class="line">        &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="built_in">free</span>(count);</span><br><span class="line">     <span class="built_in">free</span>(binary);</span><br><span class="line">     <span class="built_in">free</span>(parent_node);</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>接下来是负抽样的初始化,根据词的个数生成概率数组</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">InitUnigramTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line"> <span class="keyword">int</span> a, i;</span><br><span class="line"> <span class="keyword">long</span> <span class="keyword">long</span> train_words_pow = <span class="number">0</span>;</span><br><span class="line"> real d1, power = <span class="number">0.75</span>;</span><br><span class="line">table = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(table_size * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"> <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; vocab_size; a++) train_words_pow += <span class="built_in">pow</span>(vocab[a].cn, power);</span><br><span class="line"> i = <span class="number">0</span>;</span><br><span class="line"> d1 = <span class="built_in">pow</span>(vocab[i].cn, power) / (real)train_words_pow;</span><br><span class="line"> <span class="keyword">for</span> (a = <span class="number">0</span>; a &lt; table_size; a++) &#123;</span><br><span class="line">   table[a] = i;</span><br><span class="line">   <span class="keyword">if</span> (a / (real)table_size &gt; d1) &#123;</span><br><span class="line">     i++;</span><br><span class="line">     d1 += <span class="built_in">pow</span>(vocab[i].cn, power) / (real)train_words_pow;</span><br><span class="line">   &#125;</span><br><span class="line">      <span class="keyword">if</span> (i &gt;= vocab_size) i = vocab_size - <span class="number">1</span>;</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line">   <span class="comment">//生成table,[0,0,0,0,1,1,1,2]</span></span><br></pre></td></tr></table></figure><p>训练<br>l1 ns中表示word在sys1neg中的起始位置，主要用来获取负样本的vector<br>l2 cbow表示syn1中的非叶子节点的位置,主要用来获取非叶子节点的vector<br>taget ns中当前词语<br>label ns中当前词语的label,<br>neu1  隐藏层的vector<br>nuu1e 误差累积项</p><p>两个模型cbow和skip-<br>具体的推理过程可以看<a href="http://blog.csdn.net/itplus/article/details/37969979" title="csdn 基于hs的模型" target="_blank" rel="noopener">csdn 基于hs的模型</a><br>其中cbow的模型结构如如下文:</p><p><img src="http://ashan2012.github.io/images/word2vector/cbow.png" alt="cbow结构图"></p><p>cbow更新的伪代码如下:</p><p><img src="http://ashan2012.github.io/images/word2vector/cbowliucheng.png" alt="cbow的更新伪码"></p><p>看一下使用hs的cbow模型:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">if</span> (cbow) &#123;  <span class="comment">//train the cbow architecture</span></span><br><span class="line">     <span class="comment">// in -&gt; hidden</span></span><br><span class="line">     cw = <span class="number">0</span>;</span><br><span class="line">     <span class="keyword">for</span> (a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++) <span class="keyword">if</span> (a != window) &#123;   <span class="comment">//context(w)输入上下文相加得到隐藏层</span></span><br><span class="line">       c = sentence_position - window + a;</span><br><span class="line">       <span class="keyword">if</span> (c &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">       <span class="keyword">if</span> (c &gt;= sentence_length) <span class="keyword">continue</span>;</span><br><span class="line">       last_word = sen[c];</span><br><span class="line">       <span class="keyword">if</span> (last_word == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">       <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1[c] += syn0[c + last_word * layer1_size];</span><br><span class="line">       cw++;</span><br><span class="line">     &#125;   </span><br><span class="line">     <span class="keyword">if</span> (cw) &#123;</span><br><span class="line">       <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1[c] /= cw;   <span class="comment">//隐藏层的变量除以输入的变量个数</span></span><br><span class="line">       <span class="keyword">if</span> (hs) <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; vocab[word].codelen; d++) &#123;</span><br><span class="line">         f = <span class="number">0</span>;</span><br><span class="line">         l2 = vocab[word].point[d] * layer1_size;  <span class="comment">//非叶子节点的位置</span></span><br><span class="line">         <span class="comment">// Propagate hidden -&gt; output</span></span><br><span class="line">         <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1[c + l2];   <span class="comment">//逻辑函数的因变量,Xw*参数,即非叶子节点向量</span></span><br><span class="line">         <span class="keyword">if</span> (f &lt;= -MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">         <span class="keyword">else</span> <span class="keyword">if</span> (f &gt;= MAX_EXP) <span class="keyword">continue</span>;</span><br><span class="line">         <span class="keyword">else</span> f = expTable[(<span class="keyword">int</span>)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))];  <span class="comment">//分类的概率值</span></span><br><span class="line">         <span class="comment">// 'g' is the gradient multiplied by the learning rate</span></span><br><span class="line">         g = (<span class="number">1</span> - vocab[word].code[d] - f) * alpha;    </span><br><span class="line">         <span class="comment">// Propagate errors output -&gt; hidden</span></span><br><span class="line">         <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1[c + l2];  <span class="comment">//误差累积项，用来更新词向量，上下文词的向量</span></span><br><span class="line">         <span class="comment">// Learn weights hidden -&gt; output</span></span><br><span class="line">         <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1[c + l2] += g * neu1[c];   <span class="comment">//更新非叶子节点的向量，即参数向量</span></span><br><span class="line">       &#125;   </span><br><span class="line">      <span class="comment">//文中此处为负抽样的代码</span></span><br><span class="line">      <span class="comment">//更新所有上下文词的词向量</span></span><br><span class="line">       <span class="comment">// hidden -&gt; in</span></span><br><span class="line">       <span class="keyword">for</span> (a = b; a &lt; window * <span class="number">2</span> + <span class="number">1</span> - b; a++) <span class="keyword">if</span> (a != window) &#123;</span><br><span class="line">         c = sentence_position - window + a;</span><br><span class="line">         <span class="keyword">if</span> (c &lt; <span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">         <span class="keyword">if</span> (c &gt;= sentence_length) <span class="keyword">continue</span>;</span><br><span class="line">         last_word = sen[c];</span><br><span class="line">         <span class="keyword">if</span> (last_word == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">         <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn0[c + last_word * layer1_size] += neu1e[c]; <span class="comment">//这一步更新</span></span><br><span class="line">       &#125;</span><br></pre></td></tr></table></figure><p>负抽样的代码如下:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// NEGATIVE SAMPLING</span></span><br><span class="line"><span class="keyword">if</span> (negative &gt; <span class="number">0</span>) <span class="keyword">for</span> (d = <span class="number">0</span>; d &lt; negative + <span class="number">1</span>; d++) &#123;</span><br><span class="line">  <span class="keyword">if</span> (d == <span class="number">0</span>) &#123;</span><br><span class="line">    target = word;</span><br><span class="line">    label = <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    next_random = next_random * (<span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span>)<span class="number">25214903917</span> + <span class="number">11</span>; </span><br><span class="line">    target = table[(next_random &gt;&gt; <span class="number">16</span>) % table_size];</span><br><span class="line">    <span class="keyword">if</span> (target == <span class="number">0</span>) target = next_random % (vocab_size - <span class="number">1</span>) + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">if</span> (target == word) <span class="keyword">continue</span>;</span><br><span class="line">    label = <span class="number">0</span>;</span><br><span class="line">  &#125;   </span><br><span class="line">  l2 = target * layer1_size;</span><br><span class="line">  f = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) f += neu1[c] * syn1neg[c + l2];</span><br><span class="line">  <span class="keyword">if</span> (f &gt; MAX_EXP) g = (label - <span class="number">1</span>) * alpha;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (f &lt; -MAX_EXP) g = (label - <span class="number">0</span>) * alpha;</span><br><span class="line">  <span class="keyword">else</span> g = (label - expTable[(<span class="keyword">int</span>)((f + MAX_EXP) * (EXP_TABLE_SIZE / MAX_EXP / <span class="number">2</span>))]) * alpha;</span><br><span class="line">  <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) neu1e[c] += g * syn1neg[c + l2];</span><br><span class="line">  <span class="keyword">for</span> (c = <span class="number">0</span>; c &lt; layer1_size; c++) syn1neg[c + l2] += g * neu1[c];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>附录，推导过程：<br>负抽样方法模型推导见:<br><a href="http://blog.csdn.net/itplus/article/details/37998797" target="_blank" rel="noopener">word2vector中的负抽样</a><br>大致思路为条件为上下文，正确的词语概率大，随机负抽样的概率小</p><p><img src="http://ashan2012.github.io/images/word2vector/negtive1.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/negtive2.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/negtive3.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/negtive4.png" alt=""></p><p>cbow的推导过程:</p><p><img src="http://ashan2012.github.io/images/word2vector/cbow_1.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/cbow_1.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/cbow_1.png" alt=""></p><p><img src="http://ashan2012.github.io/images/word2vector/cbow_1.png" alt=""></p><p>写在结尾，以上推导过程来自<br><a href="http://my.csdn.net/peghoty" title="csdn 皮果提博客" target="_blank" rel="noopener">csdn 皮果提博客</a></p><p>结尾十问：<strong><em>每次写完一个模型或者一个算法都要有问题</em></strong></p><ol><li><p><strong><em>wordtovecotor训练的词向量区别于one-hot模型的地方有哪些？</em></strong></p><p> 语义特性。wordtovecotr训练的词向量表示的是语义空间的点，点之间的远近关系通过词之间的共现关系训练得到。</p></li><li><p><strong><em>训练方法上，为什么负抽样可以得到和window方法相似的效果？</em></strong></p><p> 训练空间上有语义传播性，所以在区分的不同语义的内容时，只需要区分一个就好。例如a和b关系较远，b和c关系较远，则a和c的关系也较远，这个负抽样也能达到效果的原因</p></li><li><p><strong><em>wordtovector中如何霍夫曼树是为什么可以实现等同softmax的作用？</em></strong></p><p> 以三层霍夫曼树为例，每次拆分概率为p,q,r.最后叶子节点的概率和为: p.q+p(1-q)+(1-p)r+(1-p)(1-r)=1</p></li><li><p><strong><em>wordtovector中词向量为什么可以表示语义空间的点？</em></strong></p><p> 霍夫曼树种每个节点的分裂表示的语义的一次分类，则经过整棵树的分类之后就到了叶子节点就是语义空间的一个点</p></li><li><p><strong><em>wordtovector中对高频词和低频词处理的原则，以及为什么这么做?</em></strong></p><p>低频词按照阈值丢掉，如果内容加载不下，提高阈值。<br>高频词按照一定的概率丢弃。<br>低频词本来就无法训练出来语义特征<br>高频词按照论文的解释，高频词由于共现的词很多，语义特征不具有区分性。比如训练巴黎的时候，更希望和纽约一块训练，而不是和“的”一块儿训练</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;基本的参数介绍&quot;&gt;&lt;a href=&quot;#基本的参数介绍&quot; class=&quot;headerlink&quot; title=&quot;基本的参数介绍&quot;&gt;&lt;/a&gt;基本的参数介绍&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;size&lt;/strong&gt; 输出的vector的维度&lt;/li&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="源码阅读" scheme="http://ashan2012.github.info/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="源码" scheme="http://ashan2012.github.info/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="源代码" scheme="http://ashan2012.github.info/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"/>
    
      <category term="word2vector" scheme="http://ashan2012.github.info/tags/word2vector/"/>
    
      <category term="wordtovector" scheme="http://ashan2012.github.info/tags/wordtovector/"/>
    
      <category term="词向量" scheme="http://ashan2012.github.info/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
      <category term="负抽样" scheme="http://ashan2012.github.info/tags/%E8%B4%9F%E6%8A%BD%E6%A0%B7/"/>
    
      <category term="cbow" scheme="http://ashan2012.github.info/tags/cbow/"/>
    
  </entry>
  
  <entry>
    <title>数据基础系列之一：最小二乘法</title>
    <link href="http://ashan2012.github.info/2016/11/08//blog/2016/11/08/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95.html/"/>
    <id>http://ashan2012.github.info/2016/11/08//blog/2016/11/08/最小二乘法.html/</id>
    <published>2016-11-07T16:00:00.000Z</published>
    <updated>2017-12-16T13:18:25.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>在知乎上看到一个讲最小二乘法的，感觉说的非常清楚，提炼一下</em></p><p>先从线性模型说起，输入变量为<img src="http://ashan2012.github.io/images/zxec/1.png" alt="输入变量">,线性模型为:<br><img src="http://ashan2012.github.io/images/zxec/2.png" alt="线性模型">，其中w为权重系数，最小二乘法法通过最小化预测值和真实值差值的平方和，即</p><p><img src="http://ashan2012.github.io/images/zxec/3.png" alt="线性模型"></p><p>函数为凸函数，所以可以求导获得解:<br><img src="http://ashan2012.github.io/images/zxec/4.png" alt="线性模型"><br>解为:<br><img src="http://ashan2012.github.io/images/zxec/5.png" alt="线性模型"></p><p>也可以从最大似然估计来理解最小二乘法：<br>假设误差服从高斯分布，即</p><p><img src="http://ashan2012.github.io/images/zxec/6.png" alt="线性模型"></p><p>则误差的最大似然估计为:</p><p><img src="http://ashan2012.github.io/images/zxec/6.png" alt="线性模型"></p><p>最大化似然估计，相同于最小化<img src="http://ashan2012.github.io/images/zxec/6.png" alt="线性模型">.正好为最小二乘法的优化目标。</p><p>因此，我们说在误差服从高斯分布的情况下，最小二乘法相当于最大思然估计</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;在知乎上看到一个讲最小二乘法的，感觉说的非常清楚，提炼一下&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;先从线性模型说起，输入变量为&lt;img src=&quot;http://ashan2012.github.io/images/zxec/1.png&quot; alt=&quot;输入变量&quot;&gt;,线性模型为:&lt;br
      
    
    </summary>
    
      <category term="数据基础" scheme="http://ashan2012.github.info/categories/%E6%95%B0%E6%8D%AE%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="线性回归" scheme="http://ashan2012.github.info/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="最小二乘法" scheme="http://ashan2012.github.info/tags/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95/"/>
    
      <category term="最大似然" scheme="http://ashan2012.github.info/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>源码阅读系列之jieba</title>
    <link href="http://ashan2012.github.info/2016/06/15//blog/2016/06/15/jieba%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB.html/"/>
    <id>http://ashan2012.github.info/2016/06/15//blog/2016/06/15/jieba源码阅读.html/</id>
    <published>2016-06-14T16:00:00.000Z</published>
    <updated>2017-12-16T12:31:58.000Z</updated>
    
    <content type="html"><![CDATA[<p>结巴分词用到的的原理：<br>基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)<br>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合<br>对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法</p><p>阅读源码的版本有点老，为0.38</p><p>第一点，生成trie树<br><strong>init</strong>文件中gen_pddict函数:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_pfdict</span><span class="params">(self, f)</span>:</span> </span><br><span class="line">    lfreq = &#123;&#125;           <span class="comment">#f为dict的路径</span></span><br><span class="line">    ltotal = <span class="number">0</span> </span><br><span class="line">    f_name = resolve_filename(f)</span><br><span class="line">    <span class="keyword">for</span> lineno, line <span class="keyword">in</span> enumerate(f, <span class="number">1</span>): </span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            line = line.strip().decode(<span class="string">'utf-8'</span>)  <span class="comment">#注意编码为utf-8</span></span><br><span class="line">            word, freq = line.split(<span class="string">' '</span>)[:<span class="number">2</span>]     <span class="comment">#按照空格分割，第一列word,第二列频率</span></span><br><span class="line">            freq = int(freq)</span><br><span class="line">            lfreq[word] = freq                   <span class="comment">#lfreq存储每个词对应的频率</span></span><br><span class="line">            ltotal += freq                       <span class="comment">#ltotal对应总词的数目，算词概率</span></span><br><span class="line">            <span class="keyword">for</span> ch <span class="keyword">in</span> xrange(len(word)):</span><br><span class="line">                wfrag = word[:ch + <span class="number">1</span>]</span><br><span class="line">                <span class="keyword">if</span> wfrag <span class="keyword">not</span> <span class="keyword">in</span> lfreq:</span><br><span class="line">                    lfreq[wfrag] = <span class="number">0</span> </span><br><span class="line">        <span class="keyword">except</span> ValueError:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">'invalid dictionary entry in %s at Line %s: %s'</span> % (f_name, lineno, line))</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> lfreq, ltotal</span><br></pre></td></tr></table></figure><p>这里生成的trie树就是一个字典，key为word，value为fre的字典，词语中单个汉字对应的是fre为0<br>DAG是生成一个dict+list的格式，找出所有的词,存储在DAG中，对每个sentence生成一个DAG<br>DAG {0:[1,2,3],1:[3,5],3:[4,5]},表示0-1表示一个词，0-2也是一个词，0-3也是一个词，1-3是一个词，可以看出jieba使用的是正向匹配</p><p>不使用HMM的情况下，直接计算DAG的最大概率，以每一个DAG的key找出value中中的最大概率的词语：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc</span><span class="params">(self, sentence, DAG, route)</span>:</span></span><br><span class="line">    N = len(sentence)</span><br><span class="line">    route[N] = (<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    logtotal = log(self.total)</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> xrange(N - <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        route[idx] = max((log(self.FREQ.get(sentence[idx:x + <span class="number">1</span>]) <span class="keyword">or</span> <span class="number">1</span>) -</span><br><span class="line">                          logtotal + route[x + <span class="number">1</span>][<span class="number">0</span>], x) <span class="keyword">for</span> x <span class="keyword">in</span> DAG[idx])</span><br><span class="line">        <span class="comment">#相当于计算DAG每个起点到末尾的最大概率分词。rout[idx]函数</span></span><br></pre></td></tr></table></figure><p>route对应list，每一项对应一个DAG项，结果为tuple，tuple第一项概率值，第二项为词语结尾<br>不适用HMM分词的话，直接匹配词语项:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__cut_DAG_NO_HMM</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">    DAG = self.get_DAG(sentence)</span><br><span class="line">    route = &#123;&#125;</span><br><span class="line">    self.calc(sentence, DAG, route)</span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    N = len(sentence)</span><br><span class="line">    buf = <span class="string">''</span></span><br><span class="line">    <span class="keyword">while</span> x &lt; N:</span><br><span class="line">        y = route[x][<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">        l_word = sentence[x:y]</span><br><span class="line">        <span class="keyword">if</span> re_eng.match(l_word) <span class="keyword">and</span> len(l_word) == <span class="number">1</span>:</span><br><span class="line">            buf += l_word</span><br><span class="line">            x = y    <span class="comment">#连续英文算一个词</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> buf:</span><br><span class="line">                <span class="keyword">yield</span> buf</span><br><span class="line">                buf = <span class="string">''</span></span><br><span class="line">            <span class="keyword">yield</span> l_word</span><br><span class="line">            x = y</span><br><span class="line">    <span class="keyword">if</span> buf:</span><br><span class="line">        <span class="keyword">yield</span> buf</span><br><span class="line">        buf = <span class="string">''</span></span><br></pre></td></tr></table></figure><p>使用HMM分词的函数为__cut__DAG:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__cut_DAG</span><span class="params">(self, sentence)</span>:</span></span><br><span class="line">    DAG = self.get_DAG(sentence)</span><br><span class="line">    route = &#123;&#125;</span><br><span class="line">    self.calc(sentence, DAG, route)</span><br><span class="line">    x = <span class="number">0</span></span><br><span class="line">    buf = <span class="string">''</span></span><br><span class="line">    N = len(sentence)</span><br><span class="line">    <span class="keyword">while</span> x &lt; N:</span><br><span class="line">        y = route[x][<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">        l_word = sentence[x:y]</span><br><span class="line">        <span class="keyword">if</span> y - x == <span class="number">1</span>:</span><br><span class="line">            buf += l_word     <span class="comment">#单个字连续会连在一起用HMM分词</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> buf:</span><br><span class="line">                <span class="keyword">if</span> len(buf) == <span class="number">1</span>:   <span class="comment">#如果下一个是词语，那上一个字就是单个字输出</span></span><br><span class="line">                    <span class="keyword">yield</span> buf</span><br><span class="line">                    buf = <span class="string">''</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> self.FREQ.get(buf):          <span class="comment">#连续字在词典中找不到，HMM</span></span><br><span class="line">                        recognized = finalseg.cut(buf) </span><br><span class="line">                        <span class="keyword">for</span> t <span class="keyword">in</span> recognized:</span><br><span class="line">                            <span class="keyword">yield</span> t</span><br><span class="line">                    <span class="keyword">else</span>:                            <span class="comment">#连续字在词典中找到了，输出</span></span><br><span class="line">                        <span class="keyword">for</span> elem <span class="keyword">in</span> buf:</span><br><span class="line">                            <span class="keyword">yield</span> elem</span><br><span class="line">                    buf = <span class="string">''</span></span><br><span class="line">            <span class="keyword">yield</span> l_word</span><br><span class="line">        x = y</span><br><span class="line">    <span class="keyword">if</span> buf:</span><br><span class="line">        <span class="keyword">if</span> len(buf) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">yield</span> buf</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> self.FREQ.get(buf):</span><br><span class="line">            recognized = finalseg.cut(buf)</span><br><span class="line">            <span class="keyword">for</span> t <span class="keyword">in</span> recognized:</span><br><span class="line">                <span class="keyword">yield</span> t</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> elem <span class="keyword">in</span> buf:</span><br><span class="line">                <span class="keyword">yield</span> elem</span><br></pre></td></tr></table></figure><p>其中新词的识别通过HMM实现，对要识别的字符串作为观测序列，状态集合包括(B,M,E,S)，分别对应开始，中间，结束，单字四个状态，在finalseg文件夹有线下算好的HMM模型，prob_start.py对应初始概率，prob_trans.py对应于状态转移矩阵，prob_emit.py对应观测概率。最大概率的状态序列对应为HMM的解码问题，解码使用维特比算法。就是前向算法将加变为max。</p><p>HMM函数位于finalseg文件夹下面，在<strong>init</strong>.py，如基于函数为:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__cut</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> emit_P</span><br><span class="line">    prob, pos_list = viterbi(sentence, <span class="string">'BMES'</span>, start_P, trans_P, emit_P)</span><br><span class="line">    begin, nexti = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="comment"># print pos_list, sentence</span></span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> enumerate(sentence):</span><br><span class="line">        pos = pos_list[i]</span><br><span class="line">        <span class="keyword">if</span> pos == <span class="string">'B'</span>:</span><br><span class="line">            begin = i </span><br><span class="line">        <span class="keyword">elif</span> pos == <span class="string">'E'</span>:</span><br><span class="line">            <span class="keyword">yield</span> sentence[begin:i + <span class="number">1</span>]</span><br><span class="line">            nexti = i + <span class="number">1</span> </span><br><span class="line">        <span class="keyword">elif</span> pos == <span class="string">'S'</span>:</span><br><span class="line">            <span class="keyword">yield</span> char</span><br><span class="line">            nexti = i + <span class="number">1</span> </span><br><span class="line">    <span class="keyword">if</span> nexti &lt; len(sentence):</span><br><span class="line">        <span class="keyword">yield</span> sentence[nexti:]</span><br></pre></td></tr></table></figure><p>viterbi输出标注的BMSE状态序列，按照状态序列可知分词结果:<br>维特比算法流程如下:<br><img src="http://ashan2012.github.io/images/jieba/vterbi_01.png" alt="维特比算法流程图"></p><p>具体的代码中如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(obs, states, start_p, trans_p, emit_p)</span>:</span></span><br><span class="line">    V = [&#123;&#125;]  <span class="comment"># tabular</span></span><br><span class="line">    path = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> states:  <span class="comment"># init   #开始的状态</span></span><br><span class="line">        V[<span class="number">0</span>][y] = start_p[y] + emit_p[y].get(obs[<span class="number">0</span>], MIN_FLOAT)  <span class="comment">#V存储每个时间节                    </span></span><br><span class="line">                                                                <span class="comment">#点，每个状态的概率</span></span><br><span class="line">        path[y] = [y]      <span class="comment">#path存储以y状态结束的最大概率状态序列</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> xrange(<span class="number">1</span>, len(obs)):</span><br><span class="line">        V.append(&#123;&#125;)</span><br><span class="line">        newpath = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> states:</span><br><span class="line">            em_p = emit_p[y].get(obs[t], MIN_FLOAT)</span><br><span class="line">            (prob, state) = max(</span><br><span class="line">                [(V[t - <span class="number">1</span>][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) <span class="keyword">for</span> y0 <span class="keyword">in</span> PrevStatus[y]])</span><br><span class="line">            V[t][y] = prob</span><br><span class="line">            newpath[y] = path[state] + [y] </span><br><span class="line">        path = newpath</span><br><span class="line"></span><br><span class="line">    (prob, state) = max((V[len(obs) - <span class="number">1</span>][y], y) <span class="keyword">for</span> y <span class="keyword">in</span> <span class="string">'ES'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (prob, path[state])</span><br></pre></td></tr></table></figure><p>到此为止，结巴分词的原理已经讲完！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;结巴分词用到的的原理：&lt;br&gt;基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)&lt;br&gt;采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合&lt;br&gt;对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算
      
    
    </summary>
    
      <category term="源码阅读" scheme="http://ashan2012.github.info/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="jieba" scheme="http://ashan2012.github.info/tags/jieba/"/>
    
      <category term="trie" scheme="http://ashan2012.github.info/tags/trie/"/>
    
      <category term="源码" scheme="http://ashan2012.github.info/tags/%E6%BA%90%E7%A0%81/"/>
    
      <category term="源代码" scheme="http://ashan2012.github.info/tags/%E6%BA%90%E4%BB%A3%E7%A0%81/"/>
    
      <category term="HMM" scheme="http://ashan2012.github.info/tags/HMM/"/>
    
      <category term="维特比" scheme="http://ashan2012.github.info/tags/%E7%BB%B4%E7%89%B9%E6%AF%94/"/>
    
      <category term="viterbi" scheme="http://ashan2012.github.info/tags/viterbi/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘系列之回归问题</title>
    <link href="http://ashan2012.github.info/2016/01/03//blog/2016/01/03/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98.html/"/>
    <id>http://ashan2012.github.info/2016/01/03//blog/2016/01/03/回归问题.html/</id>
    <published>2016-01-02T16:00:00.000Z</published>
    <updated>2017-12-16T13:31:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>逻辑回归主要用于分类模型以及ctr预测模型。</em></p><p>逻辑回归首先从逻辑函数讲起。</p><script type="math/tex; mode=display">\begin{eqnarray}\nabla\cdot\vec{E} &=& \frac{\rho}{\epsilon_0} \\\nabla\cdot\vec{B} &=& 0 \\\nabla\times\vec{E} &=& -\frac{\partial B}{\partial t} \\\nabla\times\vec{B} &=& \mu_0\left(\vec{J}+\epsilon_0\frac{\partial E}{\partial t} \right)\end{eqnarray}</script><p>逻辑函数的定义如下：<script type="math/tex">F(wx)=\frac{1}{1+{e^{-wx}}}</script></p><p>在逻辑回归问题里用逻辑函数表示样本被判定为正样本的概率，即P(y=+1|x)=F(wx)，则相应的判断负样本的概率为1-P(y=+1|x)。由此可知对新样本分类的问题归结为比较上述两个概率的大小。对于概率函数进行极大似然估计。依据引入的逻辑函数进行极大似然估计的步骤详见图片推导过程。 </p><p>逻辑分布理解为限制条件下的最大熵模型：<br>具体推导课件<a href="http://blog.csdn.net/dp_bupt/article/details/50568392" target="_blank" rel="noopener">csdn博客</a><br><a href="http://www.win-vector.com/dfi/LogisticRegressionMaxEnt.pdf" target="_blank" rel="noopener">推导pdf</a></p><p>逻辑回归要达到的目的如下：<br><img src="http://ashan2012.github.io/images/lr/lr_maxentroy.png" alt="逻辑回归的目标"><br>有目标函数推到出的函数分布放好为最大熵分布，也是逻辑回归的模型一般化:</p><p><img src="http://ashan2012.github.io/images/lr/maxentroy2.png" alt="逻辑回归模型一般化"></p><p>具体介绍如下：</p><p><img src="http://ashan2012.github.io/images/lr/lr-1.png" alt="逻辑回归1"></p><p><img src="http://ashan2012.github.io/images/lr/lr-2.png" alt="逻辑回归2"></p><p>BLFG有海森矩阵转换为海森矩阵的逆过程如下:</p><p><img src="http://ashan2012.github.io/images/lr/sherman_morrision_1.png" alt="sherman_morrision_1"></p><p><img src="http://ashan2012.github.io/images/lr/sherman_morrision_2.png" alt="sherman_morrision_2"></p><p>线下训练的常用的算法BFGS算法如下:</p><p><img src="http://ashan2012.github.io/images/lr/LBFGS_1.png" alt="BLFGS1"></p><p><img src="http://ashan2012.github.io/images/lr/LBFGS_2.png" alt="BLFGS2"></p><p><img src="http://ashan2012.github.io/images/lr/LBFGS_3.png" alt="BLFGS3"></p><p><img src="http://ashan2012.github.io/images/lr/LBFGS_4.png" alt="BLFGS4"></p><p><img src="http://ashan2012.github.io/images/lr/LBFGS_5.png" alt="BLFGS5"></p><p>补充LR1规则下训练算法OWL-QN:</p><p><img src="http://ashan2012.github.io/images/lr/own-qn-1.png" alt="own-qn-1"></p><p><img src="http://ashan2012.github.io/images/lr/own-qn-2.png" alt="own-qn-2"></p><p><img src="http://ashan2012.github.io/images/lr/own-qn-3.png" alt="own-qn-3"></p><p>补充trust-region-newton-method(trnm)回归:<a href="http://ashan2012.github.io/pdffile/lr/Trust-Region-Newton-Method-for-Large-Scale-Logistic.pdf" target="_blank" rel="noopener">trust-region-newton-method-for-large-scale-logistic-regression</a></p><p>补充在线训练ctr的方法:<br>参考论文:<a href="http://ashan2012.github.io/pdffile/lr/ad-click-prediction.pdf" target="_blank" rel="noopener">Ad Click Prediction-a View from the Trenches_H. Brendan McMahan_2013.pdf</a><br><a href="http://www.datakit.cn/blog/2016/05/11/ftrl.html" target="_blank" rel="noopener">具体细节参考来自datakit</a><br>具体内容如下:</p><p><img src="http://ashan2012.github.io/images/lr/ftrl_1.png" alt="ftrl_1"></p><p><img src="http://ashan2012.github.io/images/lr/ftrl_2.png" alt="ftrl_2"></p><p><img src="http://ashan2012.github.io/images/lr/ftrl_3.png" alt="ftrl_3"></p><p><img src="http://ashan2012.github.io/images/lr/ftrl_4.png" alt="ftrl_4"></p><p><img src="http://ashan2012.github.io/images/lr/ftrl_5.png" alt="ftrl_5"></p><p><img src="http://ashan2012.github.io/images/lr/ftrl_6.png" alt="ftrl_6"></p><p>附：各个优化算法参考<a href="http://blog.csdn.net/langb2014/article/details/48915425" target="_blank" rel="noopener">csdn博客</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;逻辑回归主要用于分类模型以及ctr预测模型。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;逻辑回归首先从逻辑函数讲起。&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{eqnarray}
\nabla\cdot\vec{E} &amp;=&amp; 
      
    
    </summary>
    
      <category term="数据挖掘" scheme="http://ashan2012.github.info/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="线性回归" scheme="http://ashan2012.github.info/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://ashan2012.github.info/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="单变量回归" scheme="http://ashan2012.github.info/tags/%E5%8D%95%E5%8F%98%E9%87%8F%E5%9B%9E%E5%BD%92/"/>
    
      <category term="多变量回归" scheme="http://ashan2012.github.info/tags/%E5%A4%9A%E5%8F%98%E9%87%8F%E5%9B%9E%E5%BD%92/"/>
    
      <category term="下降方法" scheme="http://ashan2012.github.info/tags/%E4%B8%8B%E9%99%8D%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘系列之特征工程</title>
    <link href="http://ashan2012.github.info/2015/12/14//blog/2015/12/14/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.html/"/>
    <id>http://ashan2012.github.info/2015/12/14//blog/2015/12/14/特征工程.html/</id>
    <published>2015-12-13T16:00:00.000Z</published>
    <updated>2017-12-16T12:43:38.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>“几乎所有的数据挖掘项目80%的工作量在于数据准备，数据准备的80%的工作量在于数据清洗”，已经不记得的这句话到底出自哪里，但是作为数据挖掘的做基础的工作，往往能对结果形成至关重要的原因。</em></p><pre><code>1.数据准备（又名特征工程）：数据准备一般指从开始抓取数据到将数据转化为合适的形式进入我们所做的模型这个阶段的工作</code></pre><p>要想做好数据挖掘的工作，必须在数据准备阶段给予足够的重视。此次博客将会详细的介绍数据准备的各个方面，但是由于博主经验有限不可能将方方面卖弄全都写到，只能抛砖引玉，希望看到博文的各个前辈高人提出宝贵的意见。</p><p>我们结合一个特征工程的整体框架图来看，更加容易理解</p><p><img src="http://ashan2012.github.io/images/20151214150719_chactor.jpg" alt="特征工程整体框架"></p><h2 id="特征使用方案"><a href="#特征使用方案" class="headerlink" title="特征使用方案"></a>特征使用方案</h2><p>首先获取特征，主要对于因变量有影响的特征都应该列入此范畴。所谓因变量就是结果，如分类结果，预测结果等。以文本分类为例，那词语，文本的长度，或者文本之中含有的特别的信息均可作为文本的特征。博主做过网站的分类，主要用到了网站的首页信息和首页中的图片的个数，链接的个数，文本长度等等均可作为特征。</p><p>其次是可用性评估，可用性评估主要是考虑几个方面，首先获得某个特征的难度，如对于网站而言，获得网站的ip，title，keyword等可以通过直接的网页分析就可获得，但是对于网站拥有的页面数，以及每个页面的相关特征就需要一定的成本。然后是特征的覆盖率。特征的覆盖率计算一般有两个作用，一个是特征选择，通过覆盖率的计算判断这个特征能否作为模型的输入参数，还有就是通过特征的覆盖率监控发现故障，如某个特征的覆盖率出现异常波动，那肯定是数据源到特征流程出现问题。最后是对数据的准确率的特征的评估。由于每个特征是通过程序自动生成，而且往往是通过少量的训练集的计算得出，则肯定不能考虑所有的情况。如网页分类中，提取网页的关键字等，大多数网页会遵循一定的规则，通过html分析得到，但是还是会有部分网站网页结构并不一样，这样在我们提取的时候的出现错误，直接影响了特征的生成。</p><h2 id="特征获取方案"><a href="#特征获取方案" class="headerlink" title="特征获取方案"></a>特征获取方案</h2><p>知道了要用哪些特征，如何得到这些特征是一个问题。一般得到特征对于网页，就是爬虫爬取，页面解析等，如资源较大，可采用分布式爬虫，如nutch，scrapy等框架，也可以自己写爬虫框架，根据需求来。至于如何存储，简单一点就是文件就可，当然做成mysql，mongodb或者直接在Hadoop上计算，博主一般通过Hadoop计算文件交互就可满足大部分需求</p><h2 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h2><p>特征处理包括特征清洗，特征预处理。特征清洗主要是又分为异常样本的处理以及数据采样。</p><p>异常值处理主要的问题在于如何发现“异常点”，这里的异常点是在数据集中与众不同的数据，使人怀疑这些数据并非随机偏差， 而是产生于完全不同的机制。常见的异常点检测算法包括：</p><ul><li>偏差检测，如聚类（异常点不在任意一个类中）、序列异常、最近邻居法、多维数据分析等</li><li>基于统计的异常点检测。在一直数据的一定分布的情况下，通过数据的变异指标对数据检测。这些异常指标包含了极差，四分位数间距，均差，标准差，变异系数等，变异指标较大表明数据分布较为分散，反之，数据比较密集。但是也存在问题就是通常我们拿到的数据并不能确定他的分布，即使可以通过数据拟合得到数据的分布，也只能在一定程度上代表整个分布，且需要付出一定的计算代价</li><li>基于距离的异常点检测;可以计算与其他点距离的平均值最大的N个数据。或者计算与第k个最近邻居距离最大的n个对象，或者与k个最近邻居的平均距离最大的n个对象。还有一些比较成熟的算法如基于索引的算法。通常情况下是构造k-d树(在最近邻中用到），设定M为异常点数据在d领域的最大对象数目，如果对象o的M+1个邻居被发现，则o为正常点。此算法复杂度为O(k*n^2),k为位数，n为数据集的数目。还有一种算法不一定常用，但是很多算法都是基于这个算法做的。首先需要两个参数pct,d.如果点O距离数据集T中pct部分的点的距离都大于d，则点O不为异常点。</li><li>基于密度的异常点检测。基于密度的异常点检测可以检测出局部异常，这是距离异常点所不能发现的。比较常用的是基于计算LOF的算法。比如一个计算相对密度的算法：定义密度为到k近邻的平均距离的倒数，公式为：</li></ul><script type="math/tex; mode=display">Density(x,k)=\left.\frac{\sum_{y \in N(x,k)}distance(x,y)^{-1}}{|N(x,k)|}\right.</script><p>&#160; &#160; &#160; &#160;其中N(x,k)是包含x的k-近邻的集合；相对密度定义为：</p><script type="math/tex; mode=display">Average relative density(x,k)=\frac{density(x,k)}{\sum_{y \in N(x,k)} density(y,k)/|N(x,k)|}</script><p>&#160; &#160; &#160; &#160;计算相对密度大的点为异常点。</p><p>数据采样主要是考虑计算量（模型不能用全部数据训练）以及实际数据的分布，如分类数据，需要正例和负例样本。如特定站点挖掘，则由于正例样本只占很小一部分，所以采样得到正例和负例样本比例靠考虑负例多一些。</p><p>数据采样过滤之后，就是根据数据得到特征，这个取决于自己定的特征。然后就需要对特征进行处理：</p><p>对于多种特征中单个特征的处理，需要经过这么几步中的一步或者几步：</p><ul><li>归一化：由于不同的特征有不同的取值范围，由于模型一般会偏向于取值范围较大的特征，为了纠正这种由于取值范围不一致导致的偏向，需要对特征做归一化。一般都是将不同取值范围的数字映射到[0,1]空间，常用的方法包括最大最小归一化，<script type="math/tex">\frac{x-x_{min}}{x_{max}-x_{min}}</script>libsvm用了这种方法做scale。还有就是排序归一化，所有值按顺序排序，赋予新值</li><li>离散化。许多特征的取值范围都是无穷的，有时候为了便于在模型中处理，需要将连续的值转化为离散值。常用方法为等值划分或者等量划分。等值划分字面意思就是按照一定固定的数值将取值范围分为一个个区间，然后取落在区间内部的数值的中位数或者中间值。等量划分主要是考虑数据分布不均匀的情况，按照数值数据划分，一般也是取数值中位数或者均值。</li><li>缩放：缩放和归一化作用一样，只不过缩放不一定缩放到0-1。缩放也可以采用最大最小方法，也可以用z-score(标准分数）:<script type="math/tex">{\frac{x-\mu}{\sigma}}</script>,<script type="math/tex">\mu</script>为特征的均值，<script type="math/tex">\sigma</script>为特征的方差，这个方法也可以作为异常点检测，一般认为z-score&gt;3的时候点为异常点。</li><li>截断。截断是指一个某个分类的特征太多肯定会影响分类的效果，所以对于一个分类，可能会做的是对所有的特征进行排序，然后对特征序列截断，只取特征序列的前几位。</li><li>数据转换。数据转换的目的为了数据更加易于区分，一方面是为了计算的方便，将特征的值做log变换。还有另外一方面是类似于box-cox变换，将不符合正态分布的特征值经过变换之后尽量符合正态分布。</li></ul><h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><p>降维单独拿出来作为一小节，因为在现在的大多数情况情况下，特征的维度都很高，要使分类达到一定的精度，降维必不可少</p><p>降维最著名的方法有两个，pca，lda。两者都是将高维数据映射到低维空间，pca将高维数据投影到低维的规则是使低维空间的数据之间的方差最大（可以理解为pca之后特征为之前特征的线性组合，之后的特征相互独立）。而lda的投影规则是使数据最容易区分。（以文本分类为例，就是将文档映射到了语义空间，而真正的语义空间比原来的特征更容易区分文档）</p><p>pca的过程：</p><ul><li>首先，定义特征矩阵A，将特征矩阵的每一项减去所在列的平均值得到新的矩阵B；</li><li>然后，求特征矩阵B的协方差矩阵C；</li><li>最后求矩阵C的特征值，并对特征值按降序排序，则前K个特征值对应的特征向量就是映射到低维空间的结果。</li></ul><p>lda的过程：本来想写一写lda的过程，但是发现对于我这种初级工程师来说有些困难，在网上查看了一些东西，发现有人可以写到比我好一百倍，只能粘来崇拜一下<a href="http://vdisk.weibo.com/s/q0sGh/1360334108?utm_source=weibolife" target="_blank" rel="noopener">lda八卦系列.pdf</a>,有心深入了解的同学可以下载下来看看。</p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>通常情况下，降维之后就到了特征选择的时候，特征选择是为了选择最优的特征子集，达到减轻计算量，提升模型精度的目的。特征选择的方法有三种。</p><ul><li>filter。filter主要是通过指标来反应特征和结果之间的关系。所以指标的选择很重要。具体是通过某个指标表示量化特征X和结果Y的关系，然后对指标来排序，取前面的k个作为特征子集。具体的指标有相关系数(皮尔逊相关系数只能衡量线性相关性，随机变量X和Y不相关并不意味二者独立）；还有假设检验的方法（最常用的<a href="http://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C" target="_blank" rel="noopener">卡方检验</a>，).当然还有互信息以及信息增益等（在决策树做特征选择一般用信息增益）。由于filter方法只考虑了X和Y之间的关系，所以计算量比较小，一般作为特征选择的预处理。</li><li>wrapper。wrapper方法主要是看特征对模型整体效果的影响。比如添加一个特征或者换删除一个特征之后模型的整体分类效果，当然还需要评估函数。(评估函数有MSE，mae，auc等。这里说一下auc，auc是指roc曲线下方的面积。roc曲线是指横坐标为FPR，纵坐标为TPR.auc的提出主要是为了克服传统模型的准确率和召回率受样本分布的不均衡性的影响)。但是这种通过添加、删除特征的方式，对于N个特征的情况，需要尝试<script type="math/tex">2^N-1</script>次，显然不科学。于是人们提出了前向选择和后向选择策略。前向选择是每次添加一个对结果最好的特征，后向选择是在使用所有特征的情况下，每次删除一个最优的选择。wrapper一般用户评估是否增加一个特征。</li><li>Embedded。Embedded特征选择中，特征选择算法本身作为组成部分嵌入到学习算法里。最典型的即决策树算法，如ID3、C4.5以及CART算<br>法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。</li></ul><h2 id="特征监控"><a href="#特征监控" class="headerlink" title="特征监控"></a>特征监控</h2><p>经过降维和特征选择之后，剩下的特征基本可以作为模型的输入了，但是需要对特征做长期的监控。当某个特别重要的特征出问题时，需要做好备案，防止灾难性结果。</p><p>关于特征工程的内容暂时写到这里，当然笔者才疏学浅，文中有不当的地方，希望各位前辈多多指教—&gt;</p><pre><code>附：True Positive （真正, TP）被模型预测为正的正样本；True Negative（真负 , TN）被模型预测为负的负样本 ；False Positive （假正, FP）被模型预测为正的负样本；False Negative（假负 , FN）被模型预测为负的正样本；1.True Positive Rate（真正率 , TPR）或灵敏度（sensitivity） ，TPR = TP /（TP + FN） 正样本预测结果数 / 正样本实际数2.True Negative Rate（真负率 , TNR）或特指度（specificity） ，TNR = TN /（TN + FP） 负样本预测结果数 / 负样本实际数 3.False Positive Rate （假正率, FPR） ，FPR = FP /（FP + TN） 被预测为正的负样本结果数 /负样本实际数 4.False Negative Rate（假负率 , FNR） FNR = FN /（TP + FN） ，被预测为负的正样本结果数 / 正样本实际数</code></pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;“几乎所有的数据挖掘项目80%的工作量在于数据准备，数据准备的80%的工作量在于数据清洗”，已经不记得的这句话到底出自哪里，但是作为数据挖掘的做基础的工作，往往能对结果形成至关重要的原因。&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1.数据准备（又名特征工程）：数据准
      
    
    </summary>
    
      <category term="数据挖掘" scheme="http://ashan2012.github.info/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="特征工程" scheme="http://ashan2012.github.info/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="数据清洗" scheme="http://ashan2012.github.info/tags/%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97/"/>
    
      <category term="特征降维" scheme="http://ashan2012.github.info/tags/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"/>
    
      <category term="增益" scheme="http://ashan2012.github.info/tags/%E5%A2%9E%E7%9B%8A/"/>
    
      <category term="pca" scheme="http://ashan2012.github.info/tags/pca/"/>
    
      <category term="互信息" scheme="http://ashan2012.github.info/tags/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    
      <category term="归一化" scheme="http://ashan2012.github.info/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    
      <category term="离散化" scheme="http://ashan2012.github.info/tags/%E7%A6%BB%E6%95%A3%E5%8C%96/"/>
    
      <category term="数据采样" scheme="http://ashan2012.github.info/tags/%E6%95%B0%E6%8D%AE%E9%87%87%E6%A0%B7/"/>
    
      <category term="相关系数" scheme="http://ashan2012.github.info/tags/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0/"/>
    
      <category term="卡方检验" scheme="http://ashan2012.github.info/tags/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
    
  </entry>
  
  <entry>
    <title>文本分类系列之一：概念及流程介绍</title>
    <link href="http://ashan2012.github.info/2015/12/09//blog/2015/12/09/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB.html/"/>
    <id>http://ashan2012.github.info/2015/12/09//blog/2015/12/09/文本分类.html/</id>
    <published>2015-12-08T16:00:00.000Z</published>
    <updated>2017-12-16T12:43:48.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>最近看了一些文本分类的东西，加上之前做过一些相关的工作，所以特别想分享一下这方面的东西。此系列博文主要是以梳理整个文本分类体系的内容为主，当然在其中参考了许多大牛的思想，希望通过这个系列的博文将文本分类梳理成为一个清晰明了的工作，当然由于博主并非大牛，许多地方可能说的并不正确，还希望看到此文的前辈朋友能给出中肯的意见。</em></p><blockquote><p>整个文本分类系列从开始介绍文本分类工程工作的流程，然后介绍了其中的一个个模块，最后会介绍在文本分类中碰到一些特殊的情况该如何处理。这篇文章主要介绍文本分类的一些基本概念以及整个模块包含哪些工作以及如何评价分类的好坏.我们从三个问题入手，什么是文本分类？文本分类怎么做？分类的效果如何评价？</p></blockquote><h2 id="什么是文本分类？"><a href="#什么是文本分类？" class="headerlink" title="什么是文本分类？"></a>什么是文本分类？</h2><pre><code>文本分类(Text Classification)是指将文本按照内容的不同判别到一个或多个预先确定的文本类别之中的过程,所以文本文类是一种有指导的映射过程（术语叫做监督学习过程），整个过程中，需要计算机通过已经标注好的数据，学习特征和类别之间的关系模型，然后以此模型来预测新的文本所属的类别</code></pre><p>我们用数学来解释这个问题。给定了文本集合<script type="math/tex">D\{d_1,d_2,..d_{\|D\|}\}</script>,类别集合 <script type="math/tex">C\{c_1,c_2,..c_{\|C\|}\}</script>,假设存在理想的映射$\phi$,既$\phi(d_1)$是已知的(对应于训练集中的结果)，我们的工作是找到一个映射$\psi$,去优化</p><script type="math/tex; mode=display"> Min\left.\sum^{|D|}_{i=1}f(\phi(d_i)-\psi(d_i))\right.</script><p>其中f为评估函数，最终目的是实现理想模型和计算模型之间的评估的损失最小</p><h2 id="文本分类有哪些特点？"><a href="#文本分类有哪些特点？" class="headerlink" title="文本分类有哪些特点？"></a>文本分类有哪些特点？</h2><h3 id="gt-特征空间高维"><a href="#gt-特征空间高维" class="headerlink" title="&gt;特征空间高维"></a>&gt;特征空间高维</h3><p>  &#160; &#160; &#160; &#160;文本数据在大多数情况下是高维的，汉字，英语组合的词至少百万甚至千万量级，一般情况下文本经过分词，去掉停用词之后剩下几十万算是很正常的，如果此时以词语作为特征，那么特征空间的维度会非常高，这么高的维度对于大多数机器学习算法来说都是一种灾难</p><h3 id="gt-特征向量稀疏"><a href="#gt-特征向量稀疏" class="headerlink" title="&gt;特征向量稀疏"></a>&gt;特征向量稀疏</h3><p>  &#160; &#160; &#160; &#160;对于文本，虽然整体的特征空间是高维的，但是对于每一项文档而言，往往只包含少数的词语。同时去掉高频词（通常所说的IDF较低）和低频词语（IDF超高，而且tf较小）之后，每个文档剩下的词语更少，既特征向量中大部分值都是0，这也是文本分类的一个显著特点</p><h3 id="gt-特征语义相关"><a href="#gt-特征语义相关" class="headerlink" title="&gt;特征语义相关"></a>&gt;特征语义相关</h3><p>   &#160; &#160; &#160; &#160;许多基于统计的算法往往假设特征之间相互独立，但是在文本分类之中，恰恰相反，词语之间高度相关，所以好的文本分类器往往考虑了词语之间的关系</p><h2 id="文本分类的基本步骤是什么？"><a href="#文本分类的基本步骤是什么？" class="headerlink" title="文本分类的基本步骤是什么？"></a>文本分类的基本步骤是什么？</h2><p>   &#160; &#160; &#160; &#160;文本分类包括获取数据，数据预处理，特征化（表示），特征选择，特征加权，训练分类器模型，评价模型，预测新的文本的结果<br>   之后的博客会将处理的每个步骤写成单独的博文。</p><h2 id="如何评价文本分类"><a href="#如何评价文本分类" class="headerlink" title="如何评价文本分类"></a>如何评价文本分类</h2><p>  &#160; &#160; &#160; &#160;分类的指标一般有三个。（当然这些评价指标一般都是在训练集上完成的，也就是通过实验所得。但是一般分类的结果好坏是有很多影响因素的，如训练集的选择的分布等，这些情况无法通过分类的指标获得）至于数据集的问题，中文文本可以采用搜狗实验室的文本材料，交叉验证做模型训练。</p><p>  评价指标包括准确率，召回率，$F_1$值<br>  文本分类的结果可以表示为：</p><p> ||                         || <em>实际属于该类的文本数</em> || <em>实际不属于该类的文本数</em> ||<br> || 分类器判别为该类的文本数   || a                    || b                     ||<br> || 分类器判别不为该类的文本数 || c                    || d                     ||</p><p> 准确率是指分类器判别为某个类别的文本中实际属于该类别的文本所占的比例，计算公式如下：</p><script type="math/tex; mode=display">p=\frac{a}{a+b}</script><p>召回率是指实际属于某个类别的文本分类器判别为该类别的文本所占的比例，计算公式如下：</p><script type="math/tex; mode=display">r=\frac{a}{a+c}</script><p>准确率表明准确性，召回率反应分类器的完备性。至于具体选择哪个指标可能根据实际情况不一样，一般通用的分类器评价好坏的时候往往选用另外一个叫做F值来评价。普通的$F_1$值为：</p><script type="math/tex; mode=display">F_1=\frac{2*p*r}{p+r}</script><p>还有一种$F_1$的变形，叫做$F_a$:</p><script type="math/tex; mode=display">F_a=\frac{(a^2+1)*p*r}{a^2*p+r}</script><p>a&gt;=0用来调整准确率和召回率之间的权重。a=0时，$F_a=p$,a=1时，$F_a=F_1$</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;最近看了一些文本分类的东西，加上之前做过一些相关的工作，所以特别想分享一下这方面的东西。此系列博文主要是以梳理整个文本分类体系的内容为主，当然在其中参考了许多大牛的思想，希望通过这个系列的博文将文本分类梳理成为一个清晰明了的工作，当然由于博主并非大牛，许多地方可能说
      
    
    </summary>
    
      <category term="数据挖掘" scheme="http://ashan2012.github.info/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    
      <category term="文本分类" scheme="http://ashan2012.github.info/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
</feed>
