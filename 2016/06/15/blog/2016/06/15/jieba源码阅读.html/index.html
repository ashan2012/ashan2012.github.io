<!DOCTYPE html>
<html lang="">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="源码阅读系列之jieba"/>




  <meta name="keywords" content="jieba trie 源码 源代码 HMM 维特比 viterbi," />





  <link rel="alternate" href="/atom.xml" title="Ashan Blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1" />



<link rel="canonical" href="http://ashan2012.github.info/2016/06/15//blog/2016/06/15/jieba源码阅读.html/"/>


<meta name="description" content="结巴分词用到的的原理：基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法 阅读源码的版本有点老，为0.38 第一点，生成trie树init文件中gen_pddict函数: 123456789101112131415161">
<meta name="keywords" content="jieba trie 源码 源代码 HMM 维特比 viterbi">
<meta property="og:type" content="article">
<meta property="og:title" content="源码阅读系列之jieba">
<meta property="og:url" content="http://ashan2012.github.info/2016/06/15//blog/2016/06/15/jieba源码阅读.html/index.html">
<meta property="og:site_name" content="Ashan Blog">
<meta property="og:description" content="结巴分词用到的的原理：基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法 阅读源码的版本有点老，为0.38 第一点，生成trie树init文件中gen_pddict函数: 123456789101112131415161">
<meta property="og:image" content="http://ashan2012.github.io/images/jieba/vterbi_01.png">
<meta property="og:updated_time" content="2017-12-10T14:15:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="源码阅读系列之jieba">
<meta name="twitter:description" content="结巴分词用到的的原理：基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法 阅读源码的版本有点老，为0.38 第一点，生成trie树init文件中gen_pddict函数: 123456789101112131415161">
<meta name="twitter:image" content="http://ashan2012.github.io/images/jieba/vterbi_01.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />
<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> 源码阅读系列之jieba - Ashan Blog </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Ashan Blog</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          源码阅读系列之jieba
        
      </h1>

      <time class="post-time">
          Jun 15 2016
      </time>
    </header>



    
            <div class="post-content">
            <p>结巴分词用到的的原理：<br>基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)<br>采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合<br>对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法</p>
<p>阅读源码的版本有点老，为0.38</p>
<p>第一点，生成trie树<br><strong>init</strong>文件中gen_pddict函数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def gen_pfdict(self, f): </span><br><span class="line">    lfreq = &#123;&#125;           #f为dict的路径</span><br><span class="line">    ltotal = 0 </span><br><span class="line">    f_name = resolve_filename(f)</span><br><span class="line">    for lineno, line in enumerate(f, 1): </span><br><span class="line">        try:</span><br><span class="line">            line = line.strip().decode(&apos;utf-8&apos;)  #注意编码为utf-8</span><br><span class="line">            word, freq = line.split(&apos; &apos;)[:2]     #按照空格分割，第一列word,第二列频率</span><br><span class="line">            freq = int(freq)</span><br><span class="line">            lfreq[word] = freq                   #lfreq存储每个词对应的频率</span><br><span class="line">            ltotal += freq                       #ltotal对应总词的数目，算词概率</span><br><span class="line">            for ch in xrange(len(word)):</span><br><span class="line">                wfrag = word[:ch + 1]</span><br><span class="line">                if wfrag not in lfreq:</span><br><span class="line">                    lfreq[wfrag] = 0 </span><br><span class="line">        except ValueError:</span><br><span class="line">            raise ValueError(</span><br><span class="line">                &apos;invalid dictionary entry in %s at Line %s: %s&apos; % (f_name, lineno, line))</span><br><span class="line">    f.close()</span><br><span class="line">    return lfreq, ltotal</span><br></pre></td></tr></table></figure>
<p>这里生成的trie树就是一个字典，key为word，value为fre的字典，词语中单个汉字对应的是fre为0<br>DAG是生成一个dict+list的格式，找出所有的词,存储在DAG中，对每个sentence生成一个DAG<br>DAG {0:[1,2,3],1:[3,5],3:[4,5]},表示0-1表示一个词，0-2也是一个词，0-3也是一个词，1-3是一个词，可以看出jieba使用的是正向匹配</p>
<p>不使用HMM的情况下，直接计算DAG的最大概率，以每一个DAG的key找出value中中的最大概率的词语：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def calc(self, sentence, DAG, route):</span><br><span class="line">    N = len(sentence)</span><br><span class="line">    route[N] = (0, 0)</span><br><span class="line">    logtotal = log(self.total)</span><br><span class="line">    for idx in xrange(N - 1, -1, -1):</span><br><span class="line">        route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -</span><br><span class="line">                          logtotal + route[x + 1][0], x) for x in DAG[idx])</span><br><span class="line">        #相当于计算DAG每个起点到末尾的最大概率分词。rout[idx]函数</span><br></pre></td></tr></table></figure>
<p>route对应list，每一项对应一个DAG项，结果为tuple，tuple第一项概率值，第二项为词语结尾<br>不适用HMM分词的话，直接匹配词语项:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def __cut_DAG_NO_HMM(self, sentence):</span><br><span class="line">    DAG = self.get_DAG(sentence)</span><br><span class="line">    route = &#123;&#125;</span><br><span class="line">    self.calc(sentence, DAG, route)</span><br><span class="line">    x = 0</span><br><span class="line">    N = len(sentence)</span><br><span class="line">    buf = &apos;&apos;</span><br><span class="line">    while x &lt; N:</span><br><span class="line">        y = route[x][1] + 1</span><br><span class="line">        l_word = sentence[x:y]</span><br><span class="line">        if re_eng.match(l_word) and len(l_word) == 1:</span><br><span class="line">            buf += l_word</span><br><span class="line">            x = y    #连续英文算一个词</span><br><span class="line">        else:</span><br><span class="line">            if buf:</span><br><span class="line">                yield buf</span><br><span class="line">                buf = &apos;&apos;</span><br><span class="line">            yield l_word</span><br><span class="line">            x = y</span><br><span class="line">    if buf:</span><br><span class="line">        yield buf</span><br><span class="line">        buf = &apos;&apos;</span><br></pre></td></tr></table></figure>
<p>使用HMM分词的函数为__cut__DAG:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">def __cut_DAG(self, sentence):</span><br><span class="line">    DAG = self.get_DAG(sentence)</span><br><span class="line">    route = &#123;&#125;</span><br><span class="line">    self.calc(sentence, DAG, route)</span><br><span class="line">    x = 0</span><br><span class="line">    buf = &apos;&apos;</span><br><span class="line">    N = len(sentence)</span><br><span class="line">    while x &lt; N:</span><br><span class="line">        y = route[x][1] + 1</span><br><span class="line">        l_word = sentence[x:y]</span><br><span class="line">        if y - x == 1:</span><br><span class="line">            buf += l_word     #单个字连续会连在一起用HMM分词</span><br><span class="line">        else:</span><br><span class="line">            if buf:</span><br><span class="line">                if len(buf) == 1:   #如果下一个是词语，那上一个字就是单个字输出</span><br><span class="line">                    yield buf</span><br><span class="line">                    buf = &apos;&apos;</span><br><span class="line">                else:</span><br><span class="line">                    if not self.FREQ.get(buf):          #连续字在词典中找不到，HMM</span><br><span class="line">                        recognized = finalseg.cut(buf) </span><br><span class="line">                        for t in recognized:</span><br><span class="line">                            yield t</span><br><span class="line">                    else:                            #连续字在词典中找到了，输出</span><br><span class="line">                        for elem in buf:</span><br><span class="line">                            yield elem</span><br><span class="line">                    buf = &apos;&apos;</span><br><span class="line">            yield l_word</span><br><span class="line">        x = y</span><br><span class="line">    if buf:</span><br><span class="line">        if len(buf) == 1:</span><br><span class="line">            yield buf</span><br><span class="line">        elif not self.FREQ.get(buf):</span><br><span class="line">            recognized = finalseg.cut(buf)</span><br><span class="line">            for t in recognized:</span><br><span class="line">                yield t</span><br><span class="line">        else:</span><br><span class="line">            for elem in buf:</span><br><span class="line">                yield elem</span><br></pre></td></tr></table></figure>
<p>其中新词的识别通过HMM实现，对要识别的字符串作为观测序列，状态集合包括(B,M,E,S)，分别对应开始，中间，结束，单字四个状态，在finalseg文件夹有线下算好的HMM模型，prob_start.py对应初始概率，prob_trans.py对应于状态转移矩阵，prob_emit.py对应观测概率。最大概率的状态序列对应为HMM的解码问题，解码使用维特比算法。就是前向算法将加变为max。</p>
<p>HMM函数位于finalseg文件夹下面，在<strong>init</strong>.py，如基于函数为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def __cut(sentence):</span><br><span class="line">    global emit_P</span><br><span class="line">    prob, pos_list = viterbi(sentence, &apos;BMES&apos;, start_P, trans_P, emit_P)</span><br><span class="line">    begin, nexti = 0, 0</span><br><span class="line">    # print pos_list, sentence</span><br><span class="line">    for i, char in enumerate(sentence):</span><br><span class="line">        pos = pos_list[i]</span><br><span class="line">        if pos == &apos;B&apos;:</span><br><span class="line">            begin = i </span><br><span class="line">        elif pos == &apos;E&apos;:</span><br><span class="line">            yield sentence[begin:i + 1]</span><br><span class="line">            nexti = i + 1 </span><br><span class="line">        elif pos == &apos;S&apos;:</span><br><span class="line">            yield char</span><br><span class="line">            nexti = i + 1 </span><br><span class="line">    if nexti &lt; len(sentence):</span><br><span class="line">        yield sentence[nexti:]</span><br></pre></td></tr></table></figure></p>
<p>viterbi输出标注的BMSE状态序列，按照状态序列可知分词结果:<br>维特比算法流程如下:<br><img src="http://ashan2012.github.io/images/jieba/vterbi_01.png" alt="维特比算法流程图"></p>
<p>具体的代码中如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def viterbi(obs, states, start_p, trans_p, emit_p):</span><br><span class="line">    V = [&#123;&#125;]  # tabular</span><br><span class="line">    path = &#123;&#125;</span><br><span class="line">    for y in states:  # init   #开始的状态</span><br><span class="line">        V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)  #V存储每个时间节                    </span><br><span class="line">                                                                #点，每个状态的概率</span><br><span class="line">        path[y] = [y]      #path存储以y状态结束的最大概率状态序列</span><br><span class="line">    for t in xrange(1, len(obs)):</span><br><span class="line">        V.append(&#123;&#125;)</span><br><span class="line">        newpath = &#123;&#125;</span><br><span class="line">        for y in states:</span><br><span class="line">            em_p = emit_p[y].get(obs[t], MIN_FLOAT)</span><br><span class="line">            (prob, state) = max(</span><br><span class="line">                [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])</span><br><span class="line">            V[t][y] = prob</span><br><span class="line">            newpath[y] = path[state] + [y] </span><br><span class="line">        path = newpath</span><br><span class="line"></span><br><span class="line">    (prob, state) = max((V[len(obs) - 1][y], y) for y in &apos;ES&apos;)</span><br><span class="line"></span><br><span class="line">    return (prob, path[state])</span><br></pre></td></tr></table></figure>
<p>到此为止，结巴分词的原理已经讲完！</p>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/jieba-trie-源码-源代码-HMM-维特比-viterbi/">jieba trie 源码 源代码 HMM 维特比 viterbi</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2016/11/08/blog/2016/11/08/最小二乘法.html/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">数据基础系列之一：最小二乘法</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2016/01/03/blog/2016/01/03/回归问题.html/">
        <span class="next-text nav-default">数据挖掘系列之回归问题</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2012 -
    
    2017
    <span class="footer-author">SuperAshan.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
