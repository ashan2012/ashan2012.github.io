<!DOCTYPE html>
<html lang="">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="源码阅读系列之textrank"/>




  <meta name="keywords" content="textrank 关键词 关键词提取 关键句子提取 自动摘要 摘要生成," />





  <link rel="alternate" href="/atom.xml" title="Ashan Blog">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1" />



<link rel="canonical" href="http://ashan2012.github.info/2017/02/21//blog/2017/02/21/textrank解读.html/"/>


<meta name="description" content="最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。  网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextR">
<meta name="keywords" content="textrank 关键词 关键词提取 关键句子提取 自动摘要 摘要生成">
<meta property="og:type" content="article">
<meta property="og:title" content="源码阅读系列之textrank">
<meta property="og:url" content="http://ashan2012.github.info/2017/02/21//blog/2017/02/21/textrank解读.html/index.html">
<meta property="og:site_name" content="Ashan Blog">
<meta property="og:description" content="最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。  网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextR">
<meta property="og:updated_time" content="2017-12-10T14:15:55.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="源码阅读系列之textrank">
<meta name="twitter:description" content="最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。  网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextR">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1" />
<link href='https://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet'>





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  



    <title> 源码阅读系列之textrank - Ashan Blog </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">Ashan Blog</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          源码阅读系列之textrank
        
      </h1>

      <time class="post-time">
          Feb 21 2017
      </time>
    </header>



    
            <div class="post-content">
            <blockquote>
<p>最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。</p>
</blockquote>
<p>网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。<br>Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextRAnk4Keyword.py主要用于提取关键词，TextRank4Sentence.py主要抽取关键的句子。util.py主要是关键词和句子排序。</p>
<p>先看提取关键词:<br>TextRank4Keyword<br><strong>init</strong>.初始化.可以在stopword.txt中添加停止词<br>进入主函数analyze中，传入参数包括要进行提取的原文，关键词边的窗口大小，是够都变为小写,节点的类型选项（影响分词结果),边的类型，pagerank的参数。</p>
<p>首先分词result = self.seg.segment(text=text, lower=lower).分词函数在Segmentation中.Segmentation初始化两个级别的分词项,WordSegmentation,SentenceSegmentation,词级别和句子级别。句子通过util.delimiters直接切割，然后词语切割直接调用结巴分词，具体结巴分词介绍可以参照:<a href="https://ashan2012.github.io/blog/2016/06/15/jieba%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB.html" title="源码阅读之jieba分词" target="_blank" rel="noopener">源码阅读之结巴分词</a>。<br>分词的时候通过allow_speech_tags控制生成词语的词性，返回的结果为dict，嵌套三层结构。第一层为选项，主要是是否使用停用词，和是否使用词性过滤两种，第二层对应句子，每个句子为word_list.</p>
<p>分词之后就进行图的构造，图构造然后排序函数为:util.sort_words.进入util.py文件中，sort_words函数中，首先是构造图:</p>
<pre><code>for word_list in _vertex_source:
    for word in word_list:
        if not word in word_index:
            word_index[word] = words_number
            index_word[words_number] = word
            words_number += 1

graph = np.zeros((words_number, words_number))

for word_list in _edge_source:
    for w1, w2 in combine(word_list, window):
        if w1 in word_index and w2 in word_index:
            index1 = word_index[w1]
            index2 = word_index[w2]
            graph[index1][index2] = 1.0 
            graph[index2][index1] = 1.0 
</code></pre><p>其中combine是构造边的函数，combine函数:<br>combine函数为word_list,中窗口window中的word之间有边。</p>
<pre><code>def combine(word_list, window = 2):
    &quot;&quot;&quot;构造在window下的单词组合，用来构造单词之间的边。

    Keyword arguments:
    word_list  --  list of str, 由单词组成的列表。
    windows    --  int, 窗口大小。
    &quot;&quot;&quot;
    if window &lt; 2: window = 2
    for x in xrange(1, window):
        if x &gt;= len(word_list):
            break
        word_list2 = word_list[x:]
        res = zip(word_list, word_list2)
        for r in res:
            yield r
</code></pre><p>构造好图矩阵之后，调用pagerank算法，使用Python中的模块networkx：</p>
<pre><code>nx_graph = nx.from_numpy_matrix(graph)
scores = nx.pagerank(nx_graph, **pagerank_config)
</code></pre><p>结果为排好序的节点（即词语)</p>
<p>提取关键句子，（自动摘由抽取式的)</p>
<p>流程和提取关键词的流程完全一致,只不过构造图的时候边的权重为两个句子的相似度，两个句子相似度函数为getsimmi</p>
<pre><code>def get_similarity(word_list1, word_list2):
    &quot;&quot;&quot;默认的用于计算两个句子相似度的函数。

    Keyword arguments:
    word_list1, word_list2  --  分别代表两个句子，都是由单词组成的列表
    &quot;&quot;&quot;
    words   = list(set(word_list1 + word_list2))
    vector1 = [float(word_list1.count(word)) for word in words]
    vector2 = [float(word_list2.count(word)) for word in words]

    vector3 = [vector1[x]*vector2[x]  for x in xrange(len(vector1))]
    vector4 = [1 for num in vector3 if num &gt; 0.]
    co_occur_num = sum(vector4)

    if abs(co_occur_num) &lt;= 1e-12:
        return 0.

    denominator = math.log(float(len(word_list1))) + math.log(float(len(word_list2))) # 分母

    if abs(denominator) &lt; 1e-12:
        return 0.

    return co_occur_num / denominator
</code></pre><p>公式为: co_word_num/(log(list1num)+log(list2num))</p>
<p>改进的方法：<br>根据pagerank的算法，可以改进的地方就是图的边，词语之间，句子之间的权重，可以使用word2vector训练得到词向量，结合位置关系计算他们之间的关系。</p>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/textrank-关键词-关键词提取-关键句子提取-自动摘要-摘要生成/">textrank 关键词 关键词提取 关键句子提取 自动摘要 摘要生成</a>
          
        </div>

        
        
  <nav class="post-nav">
    
    
      <a class="next" href="/2017/01/13/blog/2017/01/13/guesswantquery.html/">
        <span class="next-text nav-default">工作笔记之一：猜你喜欢模块</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

      </div>

      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2012 -
    
    2017
    <span class="footer-author">SuperAshan.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
